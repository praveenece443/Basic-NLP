{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import string \n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x): \n",
    "    e_x = np.exp(x - np.max(x)) \n",
    "    return e_x / e_x.sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class word2vec(object): \n",
    "    def __init__(self): \n",
    "        self.N = 80\n",
    "        self.X_train = [] \n",
    "        self.y_train = [] \n",
    "        self.window_size = 2\n",
    "        self.alpha = 0.01\n",
    "        self.words = [] \n",
    "        self.word_index = {} \n",
    "   \n",
    "    def initialize(self,V,data): \n",
    "        self.V = V \n",
    "        self.W = np.random.uniform(-0.8, 0.8, (self.V, self.N)) \n",
    "        self.W1 = np.random.uniform(-0.8, 0.8, (self.N, self.V)) \n",
    "           \n",
    "        self.words = data \n",
    "        for i in range(len(data)): \n",
    "            self.word_index[data[i]] = i \n",
    "   \n",
    "       \n",
    "    def feed_forward(self,X): \n",
    "        self.h = np.dot(self.W.T,X).reshape(self.N,1) \n",
    "        self.u = np.dot(self.W1.T,self.h) \n",
    "        #print(self.u) \n",
    "        self.y = softmax(self.u)   \n",
    "        return self.y \n",
    "           \n",
    "    def backpropagate(self,x,t): \n",
    "        e = self.y - np.asarray(t).reshape(self.V,1) \n",
    "        # e.shape is V x 1 \n",
    "        dLdW1 = np.dot(self.h,e.T) \n",
    "        X = np.array(x).reshape(self.V,1) \n",
    "        dLdW = np.dot(X, np.dot(self.W1,e).T) \n",
    "        self.W1 = self.W1 - self.alpha*dLdW1 \n",
    "        self.W = self.W - self.alpha*dLdW \n",
    "           \n",
    "    def train(self,epochs): \n",
    "        for x in range(1,epochs):         \n",
    "            self.loss = 0\n",
    "            for j in range(len(self.X_train)): \n",
    "                self.feed_forward(self.X_train[j]) \n",
    "                self.backpropagate(self.X_train[j],self.y_train[j]) \n",
    "                C = 0\n",
    "                for m in range(self.V): \n",
    "                    if(self.y_train[j][m]): \n",
    "                        self.loss += -1*self.u[m][0] \n",
    "                        C += 1\n",
    "                self.loss += C*np.log(np.sum(np.exp(self.u))) \n",
    "            print(\"epoch \",x, \" loss = \",self.loss) \n",
    "            self.alpha *= 1/( (1+self.alpha*x) ) \n",
    "              \n",
    "    def predict(self,word,number_of_predictions): \n",
    "        if word in self.words: \n",
    "            index = self.word_index[word] \n",
    "            X = [0 for i in range(self.V)] \n",
    "            X[index] = 1\n",
    "            prediction = self.feed_forward(X) \n",
    "            output = {} \n",
    "            for i in range(self.V): \n",
    "                output[prediction[i][0]] = i \n",
    "               \n",
    "            top_context_words = [] \n",
    "            for k in sorted(output,reverse=True): \n",
    "                top_context_words.append(self.words[output[k]]) \n",
    "                if(len(top_context_words)>=number_of_predictions): \n",
    "                    break\n",
    "       \n",
    "            return top_context_words \n",
    "        else: \n",
    "            print(\"Word not found in dicitonary\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(corpus): \n",
    "    stop_words = set(stopwords.words('english'))     \n",
    "    training_data = [] \n",
    "    sentences = corpus.split(\",\") \n",
    "    for i in range(len(sentences)): \n",
    "        sentences[i] = sentences[i].strip() \n",
    "        sentence = sentences[i].split() \n",
    "        x = [word.strip(string.punctuation) for word in sentence \n",
    "                                     if word not in stop_words] \n",
    "        x = [word.lower() for word in x] \n",
    "        training_data.append(x) \n",
    "    return training_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_training(sentences,w2v): \n",
    "    data = {} \n",
    "    for sentence in sentences: \n",
    "        for word in sentence: \n",
    "            if word not in data: \n",
    "                data[word] = 1\n",
    "            else: \n",
    "                data[word] += 1\n",
    "    V = len(data) \n",
    "    data = sorted(list(data.keys())) \n",
    "    vocab = {} \n",
    "    for i in range(len(data)): \n",
    "        vocab[data[i]] = i \n",
    "       \n",
    "    #for i in range(len(words)): \n",
    "    for sentence in sentences: \n",
    "        for i in range(len(sentence)): \n",
    "            center_word = [0 for x in range(V)] \n",
    "            center_word[vocab[sentence[i]]] = 1\n",
    "            context = [0 for x in range(V)] \n",
    "              \n",
    "            for j in range(i-w2v.window_size,i+w2v.window_size): \n",
    "                if i!=j and j>=0 and j<len(sentence): \n",
    "                    context[vocab[sentence[j]]] += 1\n",
    "            w2v.X_train.append(center_word) \n",
    "            w2v.y_train.append(context) \n",
    "    w2v.initialize(V,data) \n",
    "   \n",
    "    return w2v.X_train,w2v.y_train    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"\"\n",
    "data = [\"best car service \", \"cars spare parts \", \"cheap car service\"]\n",
    "for i in range(len(data)):\n",
    "    corpus += data[i]\n",
    "\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  1  loss =  65.06710545586537\n",
      "epoch  2  loss =  58.78528335377917\n",
      "epoch  3  loss =  53.9153967287341\n",
      "epoch  4  loss =  50.11095357702657\n",
      "epoch  5  loss =  47.128144900237345\n",
      "epoch  6  loss =  44.770731945954246\n",
      "epoch  7  loss =  42.87856512101086\n",
      "epoch  8  loss =  41.330393222774795\n",
      "epoch  9  loss =  40.04070910412887\n",
      "epoch  10  loss =  38.950884044203804\n",
      "epoch  11  loss =  38.020185092373254\n",
      "epoch  12  loss =  37.21923171544142\n",
      "epoch  13  loss =  36.52588161425183\n",
      "epoch  14  loss =  35.92279371050384\n",
      "epoch  15  loss =  35.395994570755654\n",
      "epoch  16  loss =  34.93400913616783\n",
      "epoch  17  loss =  34.527302832953254\n",
      "epoch  18  loss =  34.16789766328772\n",
      "epoch  19  loss =  33.8490896869573\n",
      "epoch  20  loss =  33.56522980081447\n",
      "epoch  21  loss =  33.31154749120816\n",
      "epoch  22  loss =  33.08400618847499\n",
      "epoch  23  loss =  32.87918332143255\n",
      "epoch  24  loss =  32.69417042699144\n",
      "epoch  25  loss =  32.526489868175716\n",
      "epoch  26  loss =  32.37402541656897\n",
      "epoch  27  loss =  32.234964426930304\n",
      "epoch  28  loss =  32.107749690728646\n",
      "epoch  29  loss =  31.991039351726137\n",
      "epoch  30  loss =  31.88367352066342\n",
      "epoch  31  loss =  31.784646445672877\n",
      "epoch  32  loss =  31.693083284167905\n",
      "epoch  33  loss =  31.60822068343188\n",
      "epoch  34  loss =  31.52939051368733\n",
      "epoch  35  loss =  31.456006211915252\n",
      "epoch  36  loss =  31.3875512899913\n",
      "epoch  37  loss =  31.323569639581493\n",
      "epoch  38  loss =  31.263657331247003\n",
      "epoch  39  loss =  31.207455658626575\n",
      "epoch  40  loss =  31.154645222376736\n",
      "epoch  41  loss =  31.104940884444026\n",
      "epoch  42  loss =  31.05808745264393\n",
      "epoch  43  loss =  31.013855979606173\n",
      "epoch  44  loss =  30.972040579894383\n",
      "epoch  45  loss =  30.93245568531824\n",
      "epoch  46  loss =  30.894933671780016\n",
      "epoch  47  loss =  30.859322801970997\n",
      "epoch  48  loss =  30.825485437283927\n",
      "epoch  49  loss =  30.79329647979216\n",
      "epoch  50  loss =  30.762642011345168\n",
      "epoch  51  loss =  30.733418101978717\n",
      "epoch  52  loss =  30.705529764123003\n",
      "epoch  53  loss =  30.67889003266618\n",
      "epoch  54  loss =  30.6534191539207\n",
      "epoch  55  loss =  30.629043869045482\n",
      "epoch  56  loss =  30.605696779582644\n",
      "epoch  57  loss =  30.58331578454179\n",
      "epoch  58  loss =  30.561843579961284\n",
      "epoch  59  loss =  30.541227213144538\n",
      "epoch  60  loss =  30.52141768484387\n",
      "epoch  61  loss =  30.502369593578635\n",
      "epoch  62  loss =  30.484040817053526\n",
      "epoch  63  loss =  30.46639222630826\n",
      "epoch  64  loss =  30.449387428799483\n",
      "epoch  65  loss =  30.432992537103967\n",
      "epoch  66  loss =  30.417175960352793\n",
      "epoch  67  loss =  30.40190821586731\n",
      "epoch  68  loss =  30.387161758780607\n",
      "epoch  69  loss =  30.372910827697893\n",
      "epoch  70  loss =  30.35913130468375\n",
      "epoch  71  loss =  30.345800588066858\n",
      "epoch  72  loss =  30.332897476730686\n",
      "epoch  73  loss =  30.320402064711377\n",
      "epoch  74  loss =  30.308295645059854\n",
      "epoch  75  loss =  30.296560622042403\n",
      "epoch  76  loss =  30.285180430856876\n",
      "epoch  77  loss =  30.274139464132475\n",
      "epoch  78  loss =  30.263423004560867\n",
      "epoch  79  loss =  30.253017163075626\n",
      "epoch  80  loss =  30.24290882205996\n",
      "epoch  81  loss =  30.233085583115997\n",
      "epoch  82  loss =  30.223535718978418\n",
      "epoch  83  loss =  30.214248129196854\n",
      "epoch  84  loss =  30.205212299250483\n",
      "epoch  85  loss =  30.196418262791035\n",
      "epoch  86  loss =  30.187856566741218\n",
      "epoch  87  loss =  30.179518239001595\n",
      "epoch  88  loss =  30.171394758543638\n",
      "epoch  89  loss =  30.163478027686864\n",
      "epoch  90  loss =  30.155760346378326\n",
      "epoch  91  loss =  30.14823438830861\n",
      "epoch  92  loss =  30.1408931787146\n",
      "epoch  93  loss =  30.133730073732636\n",
      "epoch  94  loss =  30.126738741178173\n",
      "epoch  95  loss =  30.119913142639085\n",
      "epoch  96  loss =  30.11324751677972\n",
      "epoch  97  loss =  30.106736363762224\n",
      "epoch  98  loss =  30.1003744306992\n",
      "epoch  99  loss =  30.09415669805961\n"
     ]
    }
   ],
   "source": [
    "training_data = preprocessing(corpus) \n",
    "w2v = word2vec() \n",
    "  \n",
    "prepare_data_for_training(training_data,w2v) \n",
    "w2v.train(epochs)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pkl_filename = \"pickle_model.pkl\"\n",
    "with open(pkl_filename, 'wb') as file:\n",
    "    pickle.dump(w2v, file)\n",
    "print(w2v.predict('car',3)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = w2v.predict(\"cars\",3)\n",
    "sentence = sent[0] + \" \" + sent[1] + \" \" + sent[2] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['spare', 'service', 'car']\n"
     ]
    }
   ],
   "source": [
    "print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Code Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import string \n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x): \n",
    "    e_x = np.exp(x - np.max(x)) \n",
    "    return e_x / e_x.sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class word2vec(object): \n",
    "    def __init__(self): \n",
    "        self.N = 80\n",
    "        self.X_train = [] \n",
    "        self.y_train = [] \n",
    "        self.window_size = 2\n",
    "        self.alpha = 0.01\n",
    "        self.words = [] \n",
    "        self.word_index = {} \n",
    "   \n",
    "    def initialize(self,V,data): \n",
    "        self.V = V \n",
    "        self.W = np.random.uniform(-0.8, 0.8, (self.V, self.N)) \n",
    "        self.W1 = np.random.uniform(-0.8, 0.8, (self.N, self.V)) \n",
    "           \n",
    "        self.words = data \n",
    "        for i in range(len(data)): \n",
    "            self.word_index[data[i]] = i \n",
    "   \n",
    "       \n",
    "    def feed_forward(self,X): \n",
    "        self.h = np.dot(self.W.T,X).reshape(self.N,1) \n",
    "        self.u = np.dot(self.W1.T,self.h) \n",
    "        #print(self.u) \n",
    "        self.y = softmax(self.u)   \n",
    "        return self.y \n",
    "           \n",
    "    def backpropagate(self,x,t): \n",
    "        e = self.y - np.asarray(t).reshape(self.V,1) \n",
    "        # e.shape is V x 1 \n",
    "        dLdW1 = np.dot(self.h,e.T) \n",
    "        X = np.array(x).reshape(self.V,1) \n",
    "        dLdW = np.dot(X, np.dot(self.W1,e).T) \n",
    "        self.W1 = self.W1 - self.alpha*dLdW1 \n",
    "        self.W = self.W - self.alpha*dLdW \n",
    "           \n",
    "    def train(self,epochs): \n",
    "        for x in range(1,epochs):         \n",
    "            self.loss = 0\n",
    "            for j in range(len(self.X_train)): \n",
    "                self.feed_forward(self.X_train[j]) \n",
    "                self.backpropagate(self.X_train[j],self.y_train[j]) \n",
    "                C = 0\n",
    "                for m in range(self.V): \n",
    "                    if(self.y_train[j][m]): \n",
    "                        self.loss += -1*self.u[m][0] \n",
    "                        C += 1\n",
    "                self.loss += C*np.log(np.sum(np.exp(self.u))) / len(self.y_train[j])\n",
    "            print(\"epoch \",x, \" loss = \",self.loss) \n",
    "            self.alpha *= 1/( (1+self.alpha*x) ) \n",
    "              \n",
    "    def predict(self,word,number_of_predictions): \n",
    "        if word in self.words: \n",
    "            index = self.word_index[word] \n",
    "            X = [0 for i in range(self.V)] \n",
    "            X[index] = 1\n",
    "            prediction = self.feed_forward(X) \n",
    "            output = {} \n",
    "            for i in range(self.V): \n",
    "                output[prediction[i][0]] = i \n",
    "               \n",
    "            top_context_words = [] \n",
    "            for k in sorted(output,reverse=True): \n",
    "                top_context_words.append(self.words[output[k]]) \n",
    "                if(len(top_context_words)>=number_of_predictions): \n",
    "                    break\n",
    "       \n",
    "            return top_context_words \n",
    "        else: \n",
    "            print(\"Word not found in dicitonary\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(corpus): \n",
    "    stop_words = set(stopwords.words('english'))     \n",
    "    training_data = [] \n",
    "    sentences = corpus.split(\",\") \n",
    "    for i in range(len(sentences)): \n",
    "        sentences[i] = sentences[i].strip() \n",
    "        sentence = sentences[i].split() \n",
    "        x = [word.strip(string.punctuation) for word in sentence \n",
    "                                     if word not in stop_words] \n",
    "        x = [word.lower() for word in x] \n",
    "        training_data.append(x) \n",
    "    return training_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_training(sentences,w2v): \n",
    "    data = {} \n",
    "    for sentence in sentences: \n",
    "        for word in sentence: \n",
    "            if word not in data: \n",
    "                data[word] = 1\n",
    "            else: \n",
    "                data[word] += 1\n",
    "    V = len(data) \n",
    "    data = sorted(list(data.keys())) \n",
    "    vocab = {} \n",
    "    for i in range(len(data)): \n",
    "        vocab[data[i]] = i \n",
    "       \n",
    "    #for i in range(len(words)): \n",
    "    for sentence in sentences: \n",
    "        for i in range(len(sentence)): \n",
    "            center_word = [0 for x in range(V)] \n",
    "            center_word[vocab[sentence[i]]] = 1\n",
    "            context = [0 for x in range(V)] \n",
    "              \n",
    "            for j in range(i-w2v.window_size,i+w2v.window_size): \n",
    "                if i!=j and j>=0 and j<len(sentence): \n",
    "                    context[vocab[sentence[j]]] += 1\n",
    "            w2v.X_train.append(center_word) \n",
    "            w2v.y_train.append(context) \n",
    "    w2v.initialize(V,data) \n",
    "   \n",
    "    return w2v.X_train,w2v.y_train    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pkl_filename = \"pickle_model.pkl\"\n",
    "with open(pkl_filename, 'rb') as file:\n",
    "    w2v = pickle.load(file) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"artificial intelligence sweeps across the world from the basic data analytics to smart security systems\"\n",
    "\n",
    "training_data = preprocessing(corpus) \n",
    "w2v = word2vec() \n",
    "  \n",
    "prepare_data_for_training(training_data,w2v) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = w2v.predict(\"artificial\",3)\n",
    "sentence = sent[0] + \" \" + sent[1] + \" \" + sent[2] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections, nltk\n",
    "# we first tokenize the text corpus\n",
    "tokens = nltk.word_tokenize(corpus)\n",
    "\n",
    "#here you construct the unigram language model \n",
    "def unigram(tokens):    \n",
    "    model = collections.defaultdict(lambda: 0.01)\n",
    "    for f in tokens:\n",
    "        try:\n",
    "            model[f] += 1\n",
    "        except KeyError:\n",
    "            model [f] = 1\n",
    "            continue\n",
    "    N = float(sum(model.values()))\n",
    "    for word in model:\n",
    "        model[word] = model[word]/N\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#computes perplexity of the unigram model on a testset  \n",
    "def perplexity(testset, model):\n",
    "    testset = testset.split()\n",
    "    perplexity = 1\n",
    "    N = 0\n",
    "    for word in testset:\n",
    "        N += 1\n",
    "        perplexity = perplexity * (1/model[word])\n",
    "    perplexity = pow(perplexity, 1/float(N)) \n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset1 = \"artificial\"\n",
    "testset2 = \"analytics data artificial\"\n",
    "\n",
    "model = unigram(tokens)\n",
    "print( perplexity(testset1, model))\n",
    "print (perplexity(testset2, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
